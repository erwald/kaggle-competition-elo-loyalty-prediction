{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz\n",
    "from feature_engineering import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_trans_df = pd.read_csv('data/unzipped/historical_transactions.csv',\n",
    "                            parse_dates=['purchase_date'])\n",
    "merchants_df = pd.read_csv('data/unzipped/merchants.csv',\n",
    "                           index_col='merchant_id')\n",
    "merch_trans_df = pd.read_csv('data/unzipped/new_merchant_transactions.csv',\n",
    "                             parse_dates=['purchase_date'])\n",
    "train_and_validation_df = pd.read_csv('data/unzipped/train.csv',\n",
    "                                      index_col='card_id',\n",
    "                                      parse_dates=['first_active_month'])\n",
    "test_df = pd.read_csv('data/unzipped/test.csv',\n",
    "                      index_col='card_id',\n",
    "                      parse_dates=['first_active_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregators = {\n",
    "    'purchase_amount': ['sum', 'mean', 'min', 'max', 'std', 'count'],\n",
    "    'installments': ['sum', 'mean', 'min', 'max', 'std'],\n",
    "    'month_lag': ['mean', 'min', 'max'],\n",
    "    'merchant_id': ['nunique'],\n",
    "    'merchant_category_id': ['nunique'],\n",
    "    'state_id': ['nunique'],\n",
    "    'city_id': ['nunique'],\n",
    "    'subsector_id': ['nunique'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_aggregated_numerical_fields(train_and_validation_df, hist_trans_df, aggregators=aggregators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_aggregated_categorical_fields(train_and_validation_df,\n",
    "                                  hist_trans_df,\n",
    "                                  column_names=['authorized_flag', 'category_1', 'category_2', 'category_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_2 and category_3 contain nan values, so let's skip those for now.\n",
    "add_top_categories(train_and_validation_df,\n",
    "                   hist_trans_df,\n",
    "                   column_names=['authorized_flag', 'category_1', 'subsector_id', 'city_id', 'state_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Separation\n",
    "\n",
    "Outliers are the ones where the target value is below -30. Most target values are clustered around 0, but then we have that bunch of values around -33, with the same value. And apparently predicting those correctly is crucial for good performance in the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validation_df['outlier'] = np.where(train_and_validation_df['target'] < -30, 'Outlier', 'Normal')\n",
    "train_and_validation_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the outliers into their own table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_df = train_and_validation_df.loc[train_and_validation_df['outlier'] == 'Outlier'].copy()\n",
    "normal_df = train_and_validation_df.loc[train_and_validation_df['outlier'] == 'Normal'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to see if there are any differences in the correlations between the full dataset and the outlier dataset. Practically everything looks exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14, 10))\n",
    "sns.heatmap(normal_df.corr(), vmin=-1, vmax=1, cmap='PiYG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14, 10))\n",
    "sns.heatmap(outlier_df.corr(), vmin=-1, vmax=1, cmap='PiYG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are no real visible differences, it's a good idea to plot the differences between the correlations. Note that the scale is different (from -0.4 to 0.4) to highlight the differences. There may be something interesting going on with `month_lag`, but otherwise the differences confirm the visual inspection of the previous graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14, 10))\n",
    "sns.heatmap(normal_df.corr() - outlier_df.corr(), vmin=-0.4, vmax=0.4, cmap='PiYG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying a Decision Tree\n",
    "\n",
    "Checking if a decision tree can be used to separate the outliers from the rest. Measure all of accuracy (proportion of correct predictions in all predictions), precision (proportion of true positives in predicted positives), and recall (proportion of predicted positives in actual positives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def draw_decision_tree(clf, feature_names):\n",
    "    dot_data = tree.export_graphviz(clf, out_file=None, feature_names=feature_names,\n",
    "                                    class_names=clf.classes_, filled=True, rounded=True,\n",
    "                                    proportion=False)\n",
    "    return graphviz.Source(dot_data)\n",
    "\n",
    "def evaluate_classification_results(clf, X_test, y_test):\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print('Accuracy: {:.4f}'.format(\n",
    "        accuracy_score(y_test, y_pred)))\n",
    "    print('Precision: {:.4f}'.format(precision_score(y_test, y_pred, pos_label='Outlier')))\n",
    "    print('Recall: {:.4f}'.format(recall_score(y_test, y_pred, pos_label='Outlier')))\n",
    "    print()\n",
    "\n",
    "    C = confusion_matrix(y_test, y_pred)\n",
    "    cm_row_labels = ['True ' + x for x in clf.classes_]\n",
    "    cm_column_labels = ['Predicted ' + x for x in clf.classes_]\n",
    "    print(pd.DataFrame(C, index=cm_row_labels, columns=cm_column_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validation_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree classifier likes only numeric features, so we need to remove all the non-numeric features (including datetime)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_df = train_and_validation_df[train_and_validation_df.columns.difference(['first_active_month',\n",
    "                                                                              'target',\n",
    "                                                                              'authorized_flag_top',\n",
    "                                                                              'category_1_top',\n",
    "                                                                              'outlier'\n",
    "                                                                             ])]\n",
    "tree_df.fillna(tree_df.mean(), inplace=True)\n",
    "data_train, data_test, label_train, label_test = train_test_split(tree_df, train_and_validation_df['outlier'], test_size=0.2)\n",
    "tree_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fiddling with the `max_depth` value gives more or less granularity to the decision tree. It doesn't take very long to build even a deep tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=9)\n",
    "clf.fit(data_train, label_train)\n",
    "\n",
    "draw_decision_tree(clf, data_train.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_classification_results(clf, data_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_classification_results(clf, data_test, label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is that a decision tree is not very good. In general, decision trees are probably a bad idea to predict rare events. By increasing the depth of the tree, we can get good-ish results on the training set, but the tree is horribly overfitted and produces bad results on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penalizing Outlier Prediction Mistakes\n",
    "\n",
    "There are [techniques for handling imbalanced classes](https://elitedatascience.com/imbalanced-classes). Let's\n",
    "try another one: penalizing mistakes made in predicting outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validation_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_train_and_validation_df = train_and_validation_df.sample(frac=0.02, random_state=281316)\n",
    "reduced_df = sampled_train_and_validation_df[sampled_train_and_validation_df.columns.difference(['first_active_month',\n",
    "                                                                                              'target',\n",
    "                                                                                              'authorized_flag_top',\n",
    "                                                                                              'category_1_top',\n",
    "                                                                                              'outlier'\n",
    "                                                                                             ])]\n",
    "reduced_df.fillna(reduced_df.mean(), inplace=True)\n",
    "data_train, data_test, label_train, label_test = train_test_split(reduced_df,\n",
    "                                                                  sampled_train_and_validation_df['outlier'],\n",
    "                                                                  test_size=0.2)\n",
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_clf = SVC(kernel='linear', class_weight='balanced', probability=True)\n",
    "svc_clf.fit(data_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_classification_results(svc_clf, data_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_classification_results(svc_clf, data_test, label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with this is that, according to the documentation, \"The fit time complexity is more than quadratic with the number of samples\", meaning that it doesn't seem possible to use a large enough dataset. In any case, this method does not seem to do so well with precision, though recall is much better than with a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling\n",
    "\n",
    "One more option is upsampling, that is, artificially inflating the number of outlier samples. Assuming there is any signal in the outlier data, this should enhance it so that it doesn't get lost in the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, validation_df = train_test_split(train_and_validation_df, test_size=0.2)\n",
    "normal_df = train_df[train_df['outlier'] == 'Normal']\n",
    "outlier_df = train_df[train_df['outlier'] == 'Outlier']\n",
    "resampled_outlier_df = resample(outlier_df,\n",
    "                               replace=True,\n",
    "                               n_samples=normal_df.outlier.count(),\n",
    "                               random_state=281316)\n",
    "resampled_df = pd.concat([normal_df, resampled_outlier_df])\n",
    "resampled_df.outlier.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = resampled_df[resampled_df.columns.difference(['first_active_month',\n",
    "                                                           'target',\n",
    "                                                           'authorized_flag_top',\n",
    "                                                           'category_1_top',\n",
    "                                                           'outlier'\n",
    "                                                          ])]\n",
    "reduced_df.fillna(reduced_df.mean(), inplace=True)\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "log_clf.fit(reduced_df, resampled_df['outlier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_classification_results(log_clf, reduced_df, resampled_df['outlier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduced_validation_df = validation_df[validation_df.columns.difference(['first_active_month',\n",
    "                                                                        'target',\n",
    "                                                                        'authorized_flag_top',\n",
    "                                                                        'category_1_top',\n",
    "                                                                        'outlier'\n",
    "                                                                       ])]\n",
    "reduced_validation_df.fillna(reduced_validation_df.mean(), inplace=True)\n",
    "evaluate_classification_results(log_clf, reduced_validation_df, validation_df['outlier'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision again appears to be a problem, but recall is quite good."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
