{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Training Data\n",
    "\n",
    "_Note! If you want to commit any changes to this document, please strip all output (Cell > Current Outputs > Clear, or set up [nbstripout](https://github.com/kynan/nbstripout) as a git filter) from this notebook before doing so. Thanks!_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Next we import the Python libraries we'll need. If any of these are missing for you, you can install them with e.g. `pip3 install pandas` on the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from fastai import *\n",
    "from fastai.tabular import *\n",
    "from fastai.metrics import *\n",
    "from feature_engineering import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load the data into Pandas data frames and look at their structure.\n",
    "\n",
    "First thing we'll do with the training data is split it into a train and validation set. (The given test set is what we'll later make our predictions on and upload, but only after we are fully satisfied with our model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_trans_df = pd.read_csv('data/unzipped/historical_transactions.csv',\n",
    "                            parse_dates=['purchase_date'])\n",
    "merchants_df = pd.read_csv('data/unzipped/merchants.csv',\n",
    "                           index_col='merchant_id')\n",
    "merch_trans_df = pd.read_csv('data/unzipped/new_merchant_transactions.csv',\n",
    "                             parse_dates=['purchase_date'])\n",
    "train_and_validation_df = pd.read_csv('data/unzipped/train.csv',\n",
    "                                      index_col='card_id',\n",
    "                                      parse_dates=['first_active_month'])\n",
    "test_df = pd.read_csv('data/unzipped/test.csv',\n",
    "                      index_col='card_id',\n",
    "                      parse_dates=['first_active_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in ['feature_1', 'feature_2', 'feature_3']:\n",
    "    train_and_validation_df[v] = train_and_validation_df[v].astype('category').cat.as_ordered()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in ['authorized_flag', 'category_1', 'category_2', 'category_3', 'merchant_id', 'merchant_category_id',\n",
    "          'subsector_id', 'city_id', 'state_id']:\n",
    "    hist_trans_df[v] = hist_trans_df[v].astype('category').cat.as_ordered()\n",
    "    merch_trans_df[v] = merch_trans_df[v].astype('category').cat.as_ordered()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Features\n",
    "\n",
    "Next we want to combine and shape all of our raw data to create useful features in the train (and validation and test) data set.\n",
    "\n",
    "Fastai has a useful helper function called `add_datepart()`, which takes a date field and turns it into a bunch of useful columns, such as \"day of week\", \"is month end\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_datepart(hist_trans_df, 'purchase_date')\n",
    "add_datepart(merch_trans_df, 'purchase_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Transaction Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll use the functions defined in `feature_engineering.py` to aggregate the historical transactions for each card into single values for that card, for instance the mean of all purchase amounts, &c.\n",
    "\n",
    "_Note: these functions can take quite a long time to complete._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_trans_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregators = {\n",
    "    'purchase_amount': ['sum', 'mean', 'min', 'max', 'std'],\n",
    "    'installments': ['sum', 'mean', 'min', 'max', 'std'],\n",
    "    'month_lag': ['mean', 'min', 'max'],\n",
    "    'merchant_id': ['nunique'],\n",
    "    'merchant_category_id': ['nunique'],\n",
    "    'state_id': ['nunique'],\n",
    "    'city_id': ['nunique'],\n",
    "    'subsector_id': ['nunique'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_aggregated_numerical_fields(train_and_validation_df, hist_trans_df, aggregators=aggregators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the categorical fields, we can't aggregate by taking the mean or sum values, so let's count the occurences of each possible categorical value instead. _(Iow, for a category that can be either YES or NO, we count the number of YESes and the number of NOs and use those values.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_aggregated_categorical_fields(train_and_validation_df,\n",
    "                                  hist_trans_df,\n",
    "                                  column_names=['authorized_flag', 'category_1', 'category_2', 'category_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_2 and category_3 contain nan values, so let's skip those for now.\n",
    "add_top_categories(train_and_validation_df,\n",
    "                   hist_trans_df,\n",
    "                   column_names=['authorized_flag', 'category_1', 'subsector_id', 'city_id', 'state_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validation_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Into Train and Validation Sets\n",
    "\n",
    "Split our data into a train test (80%) and a validation set (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, validate_df = train_test_split(train_and_validation_df, test_size=0.2, random_state=238923)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data\n",
    "\n",
    "### Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.corr().target.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(train_df.corr(), vmin=-1, vmax=1, cmap='PiYG', xticklabels=True, yticklabels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_df.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='feature_1', palette='Set2', data=train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='feature_2', palette='Set2', data=train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='feature_3', palette='Set2', data=train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='authorized_flag_top', palette='Set2', data=train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='category_1_top', palette='Set2', data=train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='subsector_id_top', palette='Set2', data=train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='city_id_top', palette='Set2', data=train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='state_id_top', palette='Set2', data=train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_df.purchase_amount_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_df.installments_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_df.month_lag_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
