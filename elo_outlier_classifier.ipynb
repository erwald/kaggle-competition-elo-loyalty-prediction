{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model to Classify Elo Customer Loyalty Outliers\n",
    "\n",
    "_Note! If you want to commit any changes to this document, please strip all output (Cell > Current Outputs > Clear, or set up [nbstripout](https://github.com/kynan/nbstripout) as a git filter) from this notebook before doing so. Thanks!_\n",
    "\n",
    "For more detailed descriptions of some of these steps, see the `elo_loyalty_prediction` notebook.\n",
    "\n",
    "PS. For now this model is not very successful when it comes to predicting outliers. Perhaps we can come back to it when we have done more feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_trans_df = pd.read_csv('data/processed/historical_transactions.csv')\n",
    "merch_trans_df = pd.read_csv('data/processed/new_merchant_transactions_with_merchants.csv')\n",
    "train_and_validation_df = pd.read_csv('data/unzipped/train.csv',\n",
    "                                      index_col='card_id',\n",
    "                                      parse_dates=['first_active_month'])\n",
    "test_df = pd.read_csv('data/unzipped/test.csv',\n",
    "                      index_col='card_id',\n",
    "                      parse_dates=['first_active_month'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.tabular import *\n",
    "from fastai.metrics import *\n",
    "from feature_engineering import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_datepart(train_and_validation_df, 'first_active_month')\n",
    "train_and_validation_df.drop(['first_active_monthDay', 'first_active_monthDayofweek',\n",
    "                              'first_active_monthDayofyear', 'first_active_monthIs_month_end',\n",
    "                              'first_active_monthIs_month_start', 'first_active_monthIs_quarter_end',\n",
    "                              'first_active_monthIs_year_end'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_datepart(test_df, 'first_active_month')\n",
    "test_df.drop(['first_active_monthDay', 'first_active_monthDayofweek', 'first_active_monthDayofyear',\n",
    "              'first_active_monthIs_month_end', 'first_active_monthIs_month_start',\n",
    "              'first_active_monthIs_quarter_end', 'first_active_monthIs_year_end'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggs = {\n",
    "    'purchase_amount': ['sum', 'mean', 'min', 'max', 'std'],\n",
    "    'installments': ['sum', 'mean', 'min', 'max', 'std'],\n",
    "    'month_lag': ['mean', 'min', 'max'],\n",
    "    'merchant_id': ['nunique'],\n",
    "    'state_id': ['nunique'],\n",
    "    'city_id': ['nunique'],\n",
    "}\n",
    "# Here are the aggregators we only want to use for the `historical_transactions` data.\n",
    "hist_trans_aggs = {\n",
    "    'merchant_category_id': ['nunique'],\n",
    "    'subsector_id': ['nunique'],\n",
    "    'elapsed_since_last_purchase': ['sum', 'mean', 'min', 'max', 'std'],\n",
    "    'elapsed_since_last_merch_purchase': ['sum', 'mean', 'min', 'max', 'std'],\n",
    "}\n",
    "# Here are the aggregators we only want to use for the `new_merchants_transactions` data.\n",
    "merch_trans_aggs = {\n",
    "    'category_1_transaction': ['nunique'],\n",
    "    'category_2': ['nunique'],\n",
    "    'category_3': ['nunique'],\n",
    "    'category_4': ['nunique'],\n",
    "    'merchant_category_id_transaction': ['nunique'],\n",
    "    'merchant_category_id_merchant': ['nunique'],\n",
    "    'merchant_group_id': ['nunique'],\n",
    "    'subsector_id_merchant': ['nunique'],\n",
    "    'category_1_merchant': ['nunique'],\n",
    "    'state_id': ['nunique'],\n",
    "    'elapsed_since_last_purchase': ['sum', 'mean', 'min', 'max', 'std'],\n",
    "    'numerical_1': ['sum', 'mean', 'min', 'max', 'std'],\n",
    "    'numerical_2': ['sum', 'mean', 'min', 'max', 'std'],\n",
    "    'avg_sales_lag3': ['sum', 'mean', 'min', 'max', 'std'],\n",
    "    'avg_purchases_lag3': ['sum', 'mean', 'min', 'max', 'std'],\n",
    "    'active_months_lag3': ['sum', 'mean', 'min', 'max', 'std'],\n",
    "    'avg_sales_lag6': ['sum', 'mean', 'min', 'max', 'std'],\n",
    "    'avg_purchases_lag6': ['sum', 'mean', 'min', 'max', 'std'],\n",
    "    'active_months_lag6': ['sum', 'mean', 'min', 'max', 'std'],\n",
    "    'avg_sales_lag12': ['sum', 'mean', 'min', 'max', 'std'],\n",
    "    'avg_purchases_lag12': ['sum', 'mean', 'min', 'max', 'std'],\n",
    "    'active_months_lag12': ['sum', 'mean', 'min', 'max', 'std'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_aggregated_numerical_fields(train_and_validation_df, hist_trans_df, aggregators={**aggs, **hist_trans_aggs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_aggregated_categorical_fields(train_and_validation_df,\n",
    "                                  hist_trans_df,\n",
    "                                  column_names=['authorized_flag', 'category_1', 'category_2', 'category_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_top_categories(train_and_validation_df,\n",
    "                   hist_trans_df,\n",
    "                   column_names=['authorized_flag', 'category_1', 'subsector_id', 'city_id', 'state_id',\n",
    "                                 'purchase_Year', 'purchase_Month', 'purchase_Week', 'purchase_Day',\n",
    "                                 'purchase_Dayofweek'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_aggregated_numerical_fields(train_and_validation_df, merch_trans_df, aggregators={**aggs, **merch_trans_aggs},\n",
    "                                prefix='merch_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, validate_df = train_test_split(train_and_validation_df, test_size=0.2, random_state=238923)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Issue\n",
    "\n",
    "If we have a look at the loyalty scores that we want to predict for this competition, we'll see that there are a bunch of outliers at around -30 loyalty. (NB. this field is likely normalised to have mean 0 and standard deviation 1, meaning these outliers are probably 0 \\[nan?] in the original data set.)\n",
    "\n",
    "If we could disregard these, our model for predicting the loyalty score would have a much easier time. But of course we don't know which of the incoming fields are outliers in this sense and which aren't. So let's try to make a classifier model to predict whether or not a sample (a `card_id`) is an outlier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_df.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call any sample with a loyalty score below -25 an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['is_outlier'] = train_df.target < -25\n",
    "validate_df['is_outlier'] = validate_df.target < -25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='is_outlier', data=train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Model\n",
    "\n",
    "Again, for more detailed comments on some of these steps, have a look at the `elo_loyalty_prediction` notebook.\n",
    "\n",
    "We will do some upsampling of the outliers in the training set â€“ basically copying those rows so that they don't drown among all the countless non-outlier samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled_train_df = train_df.copy().append([train_df[train_df.is_outlier]] * 10, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_idx = range(len(upsampled_train_df), len(upsampled_train_df) + len(validate_df)); valid_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = ['feature_1',\n",
    "                  'feature_2',\n",
    "                  'feature_3',\n",
    "                  'authorized_flag_top',\n",
    "                  'category_1_top',\n",
    "                  'subsector_id_top',\n",
    "                  'city_id_top',\n",
    "                  'state_id_top',\n",
    "                  'purchase_Year_top',\n",
    "                  'purchase_Month_top',\n",
    "                  'purchase_Week_top',\n",
    "                  'purchase_Day_top',\n",
    "                  'purchase_Dayofweek_top']\n",
    "continuous_names = ['first_active_monthYear',\n",
    "                    'first_active_monthMonth',\n",
    "                    'first_active_monthWeek',\n",
    "                    'first_active_monthIs_quarter_start',\n",
    "                    'first_active_monthIs_year_start',\n",
    "                    'first_active_monthElapsed',\n",
    "                    'purchase_amount_sum',\n",
    "                    'purchase_amount_mean',\n",
    "                    'purchase_amount_min',\n",
    "                    'purchase_amount_max',\n",
    "                    'purchase_amount_std',\n",
    "                    'installments_sum',\n",
    "                    'installments_mean',\n",
    "                    'installments_min',\n",
    "                    'installments_max',\n",
    "                    'installments_std',\n",
    "                    'month_lag_mean',\n",
    "                    'month_lag_min',\n",
    "                    'month_lag_max',\n",
    "                    'merchant_id_nunique',\n",
    "                    'state_id_nunique',\n",
    "                    'city_id_nunique',\n",
    "                    'merchant_category_id_nunique',\n",
    "                    'subsector_id_nunique',\n",
    "                    'elapsed_since_last_purchase_sum',\n",
    "                    'elapsed_since_last_purchase_mean',\n",
    "                    'elapsed_since_last_purchase_min',\n",
    "                    'elapsed_since_last_purchase_max',\n",
    "                    'elapsed_since_last_purchase_std',\n",
    "                    'elapsed_since_last_merch_purchase_sum',\n",
    "                    'elapsed_since_last_merch_purchase_mean',\n",
    "                    'elapsed_since_last_merch_purchase_min',\n",
    "                    'elapsed_since_last_merch_purchase_max',\n",
    "                    'elapsed_since_last_merch_purchase_std',\n",
    "                    'authorized_flag_Y_ratio',\n",
    "                    'category_1_N_ratio',\n",
    "                    'category_2_1.0_ratio',\n",
    "                    'category_2_3.0_ratio',\n",
    "                    'category_2_4.0_ratio',\n",
    "                    'category_2_2.0_ratio',\n",
    "                    'category_2_5.0_ratio',\n",
    "                    'category_3_A_ratio',\n",
    "                    'category_3_B_ratio',\n",
    "                    'category_3_C_ratio',\n",
    "                    'merch_purchase_amount_sum',\n",
    "                    'merch_purchase_amount_mean',\n",
    "                    'merch_purchase_amount_min',\n",
    "                    'merch_purchase_amount_max',\n",
    "                    'merch_purchase_amount_std',\n",
    "                    'merch_installments_sum',\n",
    "                    'merch_installments_mean',\n",
    "                    'merch_installments_min',\n",
    "                    'merch_installments_max',\n",
    "                    'merch_installments_std',\n",
    "                    'merch_month_lag_mean',\n",
    "                    'merch_month_lag_min',\n",
    "                    'merch_month_lag_max',\n",
    "                    'merch_merchant_id_nunique',\n",
    "                    'merch_state_id_nunique',\n",
    "                    'merch_city_id_nunique',\n",
    "                    'merch_category_1_transaction_nunique',\n",
    "                    'merch_category_2_nunique',\n",
    "                    'merch_category_3_nunique',\n",
    "                    'merch_category_4_nunique',\n",
    "                    'merch_merchant_category_id_transaction_nunique',\n",
    "                    'merch_merchant_category_id_merchant_nunique',\n",
    "                    'merch_merchant_group_id_nunique',\n",
    "                    'merch_subsector_id_merchant_nunique',\n",
    "                    'merch_category_1_merchant_nunique',\n",
    "                    'merch_elapsed_since_last_purchase_sum',\n",
    "                    'merch_elapsed_since_last_purchase_mean',\n",
    "                    'merch_elapsed_since_last_purchase_min',\n",
    "                    'merch_elapsed_since_last_purchase_max',\n",
    "                    'merch_elapsed_since_last_purchase_std',\n",
    "                    'merch_numerical_1_sum',\n",
    "                    'merch_numerical_1_mean',\n",
    "                    'merch_numerical_1_min',\n",
    "                    'merch_numerical_1_max',\n",
    "                    'merch_numerical_1_std',\n",
    "                    'merch_numerical_2_sum',\n",
    "                    'merch_numerical_2_mean',\n",
    "                    'merch_numerical_2_min',\n",
    "                    'merch_numerical_2_max',\n",
    "                    'merch_numerical_2_std',\n",
    "                    'merch_avg_sales_lag3_sum',\n",
    "                    'merch_avg_sales_lag3_mean',\n",
    "                    'merch_avg_sales_lag3_min',\n",
    "                    'merch_avg_sales_lag3_max',\n",
    "                    'merch_avg_sales_lag3_std',\n",
    "                    'merch_avg_purchases_lag3_sum',\n",
    "                    'merch_avg_purchases_lag3_mean',\n",
    "                    'merch_avg_purchases_lag3_min',\n",
    "                    'merch_avg_purchases_lag3_max',\n",
    "                    'merch_avg_purchases_lag3_std',\n",
    "                    'merch_active_months_lag3_sum',\n",
    "                    'merch_active_months_lag3_mean',\n",
    "                    'merch_active_months_lag3_min',\n",
    "                    'merch_active_months_lag3_max',\n",
    "                    'merch_active_months_lag3_std',\n",
    "                    'merch_avg_sales_lag6_sum',\n",
    "                    'merch_avg_sales_lag6_mean',\n",
    "                    'merch_avg_sales_lag6_min',\n",
    "                    'merch_avg_sales_lag6_max',\n",
    "                    'merch_avg_sales_lag6_std',\n",
    "                    'merch_avg_purchases_lag6_sum',\n",
    "                    'merch_avg_purchases_lag6_mean',\n",
    "                    'merch_avg_purchases_lag6_min',\n",
    "                    'merch_avg_purchases_lag6_max',\n",
    "                    'merch_avg_purchases_lag6_std',\n",
    "                    'merch_active_months_lag6_sum',\n",
    "                    'merch_active_months_lag6_mean',\n",
    "                    'merch_active_months_lag6_min',\n",
    "                    'merch_active_months_lag6_max',\n",
    "                    'merch_active_months_lag6_std',\n",
    "                    'merch_avg_sales_lag12_sum',\n",
    "                    'merch_avg_sales_lag12_mean',\n",
    "                    'merch_avg_sales_lag12_min',\n",
    "                    'merch_avg_sales_lag12_max',\n",
    "                    'merch_avg_sales_lag12_std',\n",
    "                    'merch_avg_purchases_lag12_sum',\n",
    "                    'merch_avg_purchases_lag12_mean',\n",
    "                    'merch_avg_purchases_lag12_min',\n",
    "                    'merch_avg_purchases_lag12_max',\n",
    "                    'merch_avg_purchases_lag12_std',\n",
    "                    'merch_active_months_lag12_sum',\n",
    "                    'merch_active_months_lag12_mean',\n",
    "                    'merch_active_months_lag12_min',\n",
    "                    'merch_active_months_lag12_max',\n",
    "                    'merch_active_months_lag12_std',]\n",
    "dep_var = 'is_outlier'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([upsampled_train_df, validate_df]).reset_index()[category_names + continuous_names + [dep_var]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TabularDataBunch.from_df('data/unzipped',\n",
    "                                df,\n",
    "                                dep_var,\n",
    "                                valid_idx=valid_idx,\n",
    "                                procs=[FillMissing, Categorify, Normalize],\n",
    "                                cat_names=category_names,\n",
    "                                cont_names=continuous_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = tabular_learner(data,\n",
    "                        layers=[200, 100],\n",
    "                        ps=[1e-2, 1e-1],\n",
    "                        emb_drop=0.04,\n",
    "                        metrics=[accuracy, Precision(), Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(3, 1e-2, wd=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions\n",
    "\n",
    "Now that we have trained our model, lets make some predictions to see whether or not our metrics lie to us.\n",
    "\n",
    "Let's only take those predictions where the model was at least 80% confident that the sample is an outlier, to increase our precision at the expense of recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, targets = [x.numpy() for x in learn.get_preds(DatasetType.Valid)]\n",
    "# Each element in prediction is an array of two values, the likelihood of \n",
    "# `False` (not an outlier) and the likelihood of `True` (an outlier).\n",
    "outlier_predictions = [x[1] > 0.9 for x in predictions]\n",
    "outlier_targets = targets == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = pd.DataFrame({'prediction': outlier_predictions, 'target': outlier_targets})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.prediction.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df[prediction_df.prediction].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate **precision** _(fraction of relevant instances among the retrieved instances)_ and **recall** _(fraction of relevant instances that have been retrieved over the total amount of relevant instances)_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_counts = prediction_df[prediction_df.prediction].target.value_counts()\n",
    "false_positives = prediction_counts[0]\n",
    "true_positives = prediction_counts[1]\n",
    "precision = true_positives / (false_positives + true_positives); precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_positives = prediction_df.target.value_counts()[1]\n",
    "true_positives = prediction_counts[1]\n",
    "recall = true_positives / total_positives; recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
