{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model to Classify Elo Customer Loyalty Outliers\n",
    "\n",
    "_Note! If you want to commit any changes to this document, please strip all output (Cell > Current Outputs > Clear, or set up [nbstripout](https://github.com/kynan/nbstripout) as a git filter) from this notebook before doing so. Thanks!_\n",
    "\n",
    "For more detailed descriptions of some of these steps, see the `elo_loyalty_prediction` notebook.\n",
    "\n",
    "PS. For now this model is not very successful when it comes to predicting outliers. Perhaps we can come back to it when we have done more feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from feature_engineering import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_trans_df = pd.read_csv('data/unzipped/historical_transactions.csv',\n",
    "                            parse_dates=['purchase_date'])\n",
    "merchants_df = pd.read_csv('data/unzipped/merchants.csv',\n",
    "                           index_col='merchant_id')\n",
    "merch_trans_df = pd.read_csv('data/unzipped/new_merchant_transactions.csv',\n",
    "                             parse_dates=['purchase_date'])\n",
    "train_and_validation_df = pd.read_csv('data/unzipped/train.csv',\n",
    "                                      index_col='card_id',\n",
    "                                      parse_dates=['first_active_month'])\n",
    "test_df = pd.read_csv('data/unzipped/test.csv',\n",
    "                      index_col='card_id',\n",
    "                      parse_dates=['first_active_month'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_aggregated_numerical_fields(train_and_validation_df,\n",
    "                                hist_trans_df,\n",
    "                                column_names=['purchase_amount', 'installments', 'month_lag'],\n",
    "                                aggregator=np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_aggregated_numerical_fields(train_and_validation_df,\n",
    "                                hist_trans_df,\n",
    "                                column_names=['purchase_amount', 'installments'],\n",
    "                                aggregator=np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_aggregated_categorical_fields(train_and_validation_df,\n",
    "                                  hist_trans_df,\n",
    "                                  column_names=['authorized_flag', 'category_1', 'category_2', 'category_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, validate_df = train_test_split(train_and_validation_df, test_size=0.2, random_state=238923)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Issue\n",
    "\n",
    "If we have a look at the loyalty scores that we want to predict for this competition, we'll see that there are a bunch of outliers at around -30 loyalty. (NB. this field is likely normalised to have mean 0 and standard deviation 1, meaning these outliers are probably 0 \\[nan?] in the original data set.)\n",
    "\n",
    "If we could disregard these, our model for predicting the loyalty score would have a much easier time. But of course we don't know which of the incoming fields are outliers in this sense and which aren't. So let's try to make a classifier model to predict whether or not a sample (a `card_id`) is an outlier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_df.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call any sample with a loyalty score below -25 an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['is_outlier'] = train_df.target < -25\n",
    "validate_df['is_outlier'] = validate_df.target < -25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='is_outlier', data=train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Model\n",
    "\n",
    "Again, for more detailed comments on some of these steps, have a look at the `elo_loyalty_prediction` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in ['feature_1', 'feature_2', 'feature_3', 'is_outlier']:\n",
    "    train_df[v] = train_df[v].astype('category').cat.as_ordered()\n",
    "    validate_df[v] = validate_df[v].astype('category').cat.as_ordered()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.tabular import *\n",
    "from fastai.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_idx = range(len(train_and_validation_df) - len(validate_df), len(train_and_validation_df)); valid_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = ['feature_1', 'feature_2', 'feature_3']\n",
    "dep_var = 'is_outlier'\n",
    "continuous_names = [col for col in train_df.columns if col not in (\n",
    "    ['first_active_month', 'target'] + category_names + [dep_var])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df, validate_df]).reset_index()[category_names + continuous_names + [dep_var]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TabularDataBunch.from_df('data/unzipped',\n",
    "                                df,\n",
    "                                dep_var,\n",
    "                                valid_idx=valid_idx,\n",
    "                                procs=[FillMissing, Categorify, Normalize],\n",
    "                                cat_names=category_names,\n",
    "                                cont_names=continuous_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = tabular_learner(data,\n",
    "                        layers=[200,100],\n",
    "                        ps=[1e-3, 1e-2],\n",
    "                        emb_drop=0.05,\n",
    "                        metrics=[accuracy, Precision(), Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(3, 1e-3, wd=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions\n",
    "\n",
    "Now that we have trained our model, lets make some predictions to see whether or not our metrics lie to us.\n",
    "\n",
    "Even though our model never assigns over 50% probability that any row is an outlier, we can consider, for instance, all samples for which the probability is over 5% to be an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, targets = [x.numpy() for x in learn.get_preds(DatasetType.Valid)]\n",
    "# Each element in prediction is an array of two values, the likelihood of \n",
    "# `False` (not an outlier) and the likelihood of `True` (an outlier).\n",
    "outlier_predictions = [x[1] > 0.05 for x in predictions]\n",
    "outlier_targets = targets == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = pd.DataFrame({'prediction': outlier_predictions, 'target': outlier_targets})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.prediction.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df[prediction_df.prediction].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate **precision** _(fraction of relevant instances among the retrieved instances)_ and **recall** _(fraction of relevant instances that have been retrieved over the total amount of relevant instances)_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_counts = prediction_df[prediction_df.prediction].target.value_counts()\n",
    "false_positives = prediction_counts[0]\n",
    "true_positives = prediction_counts[1]\n",
    "precision = true_positives / (false_positives + true_positives); precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_positives = prediction_df.target.value_counts()[1]\n",
    "true_positives = prediction_counts[1]\n",
    "recall = true_positives / total_positives; recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
